1. (잠만보(jerry)) 상세한 masking 을 입력의 어디에 주는지? 마스킹된 입력은 recuurent하게 결과를 생성하는가? 
encoder와 decoder에서 원래 시퀀스와 출력 시퀀스의 입력 길이가 맞지 않는데 모델은 이것을 어떻게 프로세스하는가
2. (잠만보(jerry)) 단어의 임베딩 벡터의 크기가 일정한가? 문장의 임베딩 벡터의 크기가 일정한가? 단어의 임베딩 벡터의 크기가 일정하다면 이것은 Q,K,V로 어떻게 계산되는가? -> 단어의 임베딩 벡터 크기가 일정할 것, 시퀀스 크기 N과 잠재채원 d로 한 번 매핑하고 이를 Q,K,V로 프로젝션해서 사용할텐데, 이 과정은 수도코드로 쓰면?
3. (잠만보(jerry)) positional embedding에서의 linear 결합이란?
